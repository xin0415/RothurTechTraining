kafka: https://www.youtube.com/watch?v=U17DtHLOsTU&list=PLGRDMO4rOGcNLwoack4ZiTyewUcF6y6BU
https://kafka.apache.org/quickstart
https://spring.io/projects/spring-kafka
https://docs.spring.io/spring-kafka/reference/index.html
 Message System
	why message system
	Message System Model
		Point to point model:       producer -> message queue -> consumer   after customer consume the message, it will send back acknowledge message to message queue. After message queue receive acknowledge, it will delete message
		each message can only consume by one customer

		publish subscribe model     producer publish topic into mq, consumer will subscribe topic from mq, but message topic in mq won't be deleted. It will delete when it is expired
	
	Kafka Architecture
		producer            producer send the message to the leader node
		consumer
		broker              each broker has partition, replica to backup the data, follower will backup data from leader, never put leader and follower in one broker, in case one broker is down
		topic (partition/replica) leader/follower           topic stored in broker
		message (key, value) -> Apache Avro for serialization  key is used to local partition (where to store value), we don't send the message directly, we need to serialize the data
		batch   message don't send to partition directly, it will put into batch first, then producer will send the whole batch to partition, it can improve performance
		all the consumers in one consumer group will work for similar functionality
		all the consumers in one consumer group should consume different partition
		zookeeper help to manage the components in kafka
	producer
		if partition, send to partition directly            - if you provide partition number directly, it will send to the partition
		if no partition, if key, module operation -> find the partition number              - if not partition number, used key for module operation to find the partition number       - key is from message
		if no partition, if no key, random generate number -> partition                     - if not partition number, if not key, random generate number for module operation to find the partition number. if it receive the message without partition number and key next time, it will increase this random number + 1, then for module operation to find the partition number


    producer send data to partition, it doesn't care whether partition receive data or not
    producer send message to partition leader node, it will set acks =1, after leader node receive and save the data, follower node backup the message from leader node. the leader node will response the acks

		acknowledge
			acks = 0, don't wait ack
			acks = 1, wait for ack from leader
			acks = -1, wait leader and follows backup the data, ISR


	Consumer: 
		offset: consumer record the last reading location
		for example, you have 10 message in partition 1, consumerA consume 5 data from partition 1, then offset =5, consumerB still want to consume partition 1, it will start from 6 because offset is 5

Project demo

Interview question: 
	message accumulated in Kafka, consumer can't consume all the data on time, what should you do?
	we can add more consumer, it can have more consumer to consume the data. or more partition or increase batch size
	how does Kafka deal with the expired data?
		log.cleanup.policy=delete
		log.cleanup.policy=compact                  compact data to clean up data
	data volume in Kafka?
		daily total data size: 100 GB
		each log: 0.5k - 2k (avg 1k)
		daily log number: 100 Million logs = 10^8
		every second: 10^8/24/60/60 = 1150 logs/s
		avg: 1150 log/s
		highest: 1150 * (2-20)= 2300-23000logs
		lowest: 50
		data volume per second: 1MB/s -> 2MB-20MB
	how to calculate partition number?
		total throughput: Tt (MB/s)         Tt = total throughput
		Producer: Tp (MB/s)                 Tp = throughput producer
		Consumer: Tc (MB/s)                 Tc = throughput consumer
		partition number = Tt / min(Tp, Tc)

		ex:
		producer Tp = 20 MB/s
		consumer Tc = 50 m/s
		total Tt = 100m/s
		partition number = 100 / 20 = 5

RabbitMQ, JMQP, SNS/SQS

Kafka Cluster
Since Kafka is a distributed system, it act as a cluster. A Kafka cluster consists of a set of brokers. A cluster has a minimum of 3 brokers.

Kafka Broker
The broker is the Kafka server. It's just a meaningful name given to the Kafka server. And this name makes sense as well because all that Kafka does is act as
a message broker between producer and consumer.
The producer and consumer don't interact directly. They use Kafka server as an agent or a broker to exchange messages.

Producer
Producer is an application that sends messages. It does not send messages directly to the recipient. It send messages only to the Kafka server.

Consumer
Consumer is an application that reads messages from the Kafka server.
If producers are sending data, they must be sending it to someone, right? The consumers are the recipients. But remember that the producers don't send data to a
recipient address. They just send it to Kafka server. And anyone who is interested in that data can come forward and take it from Kafka server. So, any application that
requests data from a Kafka server is a consumer, and they can ask for data send by any producer provided they have permissions to read it.

Kafka Topic
We learned that producer sends data to the Kafka broker. Then a consumer can ask for data from the Kafka broker. But the question is, Which data? We need to have some
identification mechanism to request data from a broker. There comes the notion of the topic.
Â· Topic is like a table in database or folder in a file system.
. Topic is identified by a name.
. You can have any number of topics.

Kafka Partitions
Kafka topics are divided into a number of partitions, which contain records in an unchangeable sequence.
Kafka Brokers will store messages for a topic. But the capacity of data can be enormous and it may not be possible to store in a single computer. Therefore it will partitioned into multiple parts and
distributed among multiple computers, since Kafka is a distributed system.
			
Offsets
Offset is a sequence of ids given to messages as the arrive at a partition. Once the offset is assigned it will never be changed. The first message gets an offset zero. The next
message receives an offset one and so on.

Consumer Groups
A consumer group contains one or more consumers working together to process the messages.

--------------------------------------------------------------------------------------
Configure Kafka Producer and Consumer for JSON Serializer and Deserializer
How to send and receive a Java Object as a JSON bytell to and from Apache Kafka.

Apache Kafka stores and transports bytel]. There are a number of built-in serializers and deserializers but it doesn't include any for JSON. Spring Kafka created a
JsonSerializer and JsonDeserializer which we can use to convert Java Objects to and from JSON.

We'll send a Java Object as JSON bytell to a Kafka Topic using a JsonSerializer. Afterward, we'll configure how to receive a JSON bytell and automatically convert it
to a Java Object using a JsonDeserializer.
------------------------------------------------------------------------------------------

Kafka run in windows
STEP 1: DOWNLOAD AND INSTALL KAFKA
https://dlcdn.apache.org/kafka/3.2.0/kafka 2.13-3.2.0.tgz

STEP 2: START THE KAFKA ENVIRONMENT
# Start the ZooKeeper service
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties

# Start the Kafka broker service
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\kafka-server-start.bat .\config\server.properties

STEP 3: CREATE A TOPIC TO STORE YOUR EVENTS
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\kafka-topics.bat --create --topic topic_demo --bootstrap-server localhost:9092

STEP 4: WRITE SOME EVENTS INTO THE TOPIC
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\kafka-console-producer.bat --topic topic_demo --bootstrap-server localhost:9092
>hello world
>topic demo

STEP 5: READ THE EVENTS
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\kafka-console-consumer.bat --topic topic_demo --from-beginning --bootstrap-server localhost:9092
hello world
topic demo

Run the following command in the new terminal
Read message from kafka after send the message from producer
bin/kafka-console-consumer.sh --topic TopicName --from-beginning --bootstrap-server localhost:9092

---------------------------------------------------------
What is event-driven architecture?
Event-driven architecture (EDA) is a software design pattern in which decoupled applications can asynchronously publish and subscribe to events via an event broker/message broker.

In an Event-Driven Architecture, applications communicate with each other by sending and/or receiving events or messages

Event-driven architecture is often referred to as "asynchronous" communication.

Event-driven apps can be created in any programming language because event- driven is a programming approach, not a language.

An event-driven architecture is loosely coupled.
---------------------------------------------------------