kafka: https://www.youtube.com/watch?v=U17DtHLOsTU&list=PLGRDMO4rOGcNLwoack4ZiTyewUcF6y6BU
https://kafka.apache.org/quickstart
https://spring.io/projects/spring-kafka
https://docs.spring.io/spring-kafka/reference/index.html
 Message System
	why message system
	Message System Model
		Point to point model:       producer -> message queue -> consumer   after customer consume the message, it will send back acknowledge message to message queue. After message queue receive acknowledge, it will delete message
		each message can only consume by one customer

		publish subscribe model     producer publish topic into mq, consumer will subscribe topic from mq, but message topic in mq won't be deleted. It will delete when it is expired
	
	Kafka Architecture
		producer            producer send the message to the leader node
		consumer
		broker              each broker has partition, replica to backup the data, follower will backup data from leader, never put leader and follower in one broker, in case one broker is down
		topic (partition/replica) leader/follower           topic stored in broker
		message (key, value) -> Apache Avro for serialization  key is used to local partition (where to store value), we don't send the message directly, we need to serialize the data
		batch   message don't send to partition directly, it will put into batch first, then producer will send the whole batch to partition, it can improve performance
		all the consumers in one consumer group will work for similar functionality
		all the consumers in one consumer group should consume different partition
		zookeeper help to manage the components in kafka
	producer
		if partition, send to partition directly            - if you provide partition number directly, it will send to the partition
		if no partition, if key, module operation -> find the partition number              - if not partition number, used key for module operation to find the partition number       - key is from message
		if no partition, if no key, random generate number -> partition                     - if not partition number, if not key, random generate number for module operation to find the partition number. if it receive the message without partition number and key next time, it will increase this random number + 1, then for module operation to find the partition number


    producer send data to partition, it doesn't care whether partition receive data or not
    producer send message to partition leader node, it will set acks =1, after leader node receive and save the data, follower node backup the message from leader node. the leader node will response the acks

		acknowledge
			acks = 0, don't wait ack
			acks = 1, wait for ack from leader
			acks = -1, wait leader and follows backup the data, ISR


	Consumer: 
		offset: consumer record the last reading location
		for example, you have 10 message in partition 1, consumerA consume 5 data from partition 1, then offset =5, consumerB still want to consume partition 1, it will start from 6 because offset is 5

Project demo

Interview question: 
	message accumulated in Kafka, consumer can't consume all the data on time, what should you do?
	we can add more consumer, it can have more consumer to consume the data. or more partition or increase batch size
	how does Kafka deal with the expired data?
		log.cleanup.policy=delete
		log.cleanup.policy=compact                  compact data to clean up data
	data volume in Kafka?
		daily total data size: 100 GB
		each log: 0.5k - 2k (avg 1k)
		daily log number: 100 Million logs = 10^8
		every second: 10^8/24/60/60 = 1150 logs/s
		avg: 1150 log/s
		highest: 1150 * (2-20)= 2300-23000logs
		lowest: 50
		data volume per second: 1MB/s -> 2MB-20MB
	how to calculate partition number?
		total throughput: Tt (MB/s)         Tt = total throughput
		Producer: Tp (MB/s)                 Tp = throughput producer
		Consumer: Tc (MB/s)                 Tc = throughput consumer
		partition number = Tt / min(Tp, Tc)

		ex:
		producer Tp = 20 MB/s
		consumer Tc = 50 m/s
		total Tt = 100m/s
		partition number = 100 / 20 = 5

RabbitMQ, JMQP, SNS/SQS

Kafka Cluster
Since Kafka is a distributed system, it act as a cluster. A Kafka cluster consists of a set of brokers. A cluster has a minimum of 3 brokers.

Kafka Broker
The broker is the Kafka server. It's just a meaningful name given to the Kafka server. And this name makes sense as well because all that Kafka does is act as
a message broker between producer and consumer.
The producer and consumer don't interact directly. They use Kafka server as an agent or a broker to exchange messages.

Producer
Producer is an application that sends messages. It does not send messages directly to the recipient. It send messages only to the Kafka server.

Consumer
Consumer is an application that reads messages from the Kafka server.
If producers are sending data, they must be sending it to someone, right? The consumers are the recipients. But remember that the producers don't send data to a
recipient address. They just send it to Kafka server. And anyone who is interested in that data can come forward and take it from Kafka server. So, any application that
requests data from a Kafka server is a consumer, and they can ask for data send by any producer provided they have permissions to read it.

Kafka Topic
We learned that producer sends data to the Kafka broker. Then a consumer can ask for data from the Kafka broker. But the question is, Which data? We need to have some
identification mechanism to request data from a broker. There comes the notion of the topic.
Â· Topic is like a table in database or folder in a file system.
. Topic is identified by a name.
. You can have any number of topics.

Kafka Partitions
Kafka topics are divided into a number of partitions, which contain records in an unchangeable sequence.
Kafka Brokers will store messages for a topic. But the capacity of data can be enormous and it may not be possible to store in a single computer. Therefore it will partitioned into multiple parts and
distributed among multiple computers, since Kafka is a distributed system.
			
Offsets
Offset is a sequence of ids given to messages as the arrive at a partition. Once the offset is assigned it will never be changed. The first message gets an offset zero. The next
message receives an offset one and so on.

Consumer Groups
A consumer group contains one or more consumers working together to process the messages.

--------------------------------------------------------------------------------------
Configure Kafka Producer and Consumer for JSON Serializer and Deserializer
How to send and receive a Java Object as a JSON bytell to and from Apache Kafka.

Apache Kafka stores and transports bytel]. There are a number of built-in serializers and deserializers but it doesn't include any for JSON. Spring Kafka created a
JsonSerializer and JsonDeserializer which we can use to convert Java Objects to and from JSON.

We'll send a Java Object as JSON bytell to a Kafka Topic using a JsonSerializer. Afterward, we'll configure how to receive a JSON bytell and automatically convert it
to a Java Object using a JsonDeserializer.
------------------------------------------------------------------------------------------

Kafka run in windows
STEP 1: DOWNLOAD AND INSTALL KAFKA
https://dlcdn.apache.org/kafka/3.2.0/kafka 2.13-3.2.0.tgz

STEP 2: START THE KAFKA ENVIRONMENT
# Start the ZooKeeper service
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties

# Start the Kafka broker service
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\kafka-server-start.bat .\config\server.properties

STEP 3: CREATE A TOPIC TO STORE YOUR EVENTS
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\kafka-topics.bat --create --topic topic_demo --bootstrap-server localhost:9092

STEP 4: WRITE SOME EVENTS INTO THE TOPIC
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\kafka-console-producer.bat --topic topic_demo --bootstrap-server localhost:9092
>hello world
>topic demo

STEP 5: READ THE EVENTS
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\kafka-console-consumer.bat --topic topic_demo --from-beginning --bootstrap-server localhost:9092
hello world
topic demo

Run the following command in the new terminal
Read message from kafka after send the message from producer
bin/kafka-console-consumer.sh --topic TopicName --from-beginning --bootstrap-server localhost:9092

---------------------------------------------------------
What is event-driven architecture?
Event-driven architecture (EDA) is a software design pattern in which decoupled applications can asynchronously publish and subscribe to events via an event broker/message broker.

In an Event-Driven Architecture, applications communicate with each other by sending and/or receiving events or messages

Event-driven architecture is often referred to as "asynchronous" communication.

Event-driven apps can be created in any programming language because event- driven is a programming approach, not a language.

An event-driven architecture is loosely coupled.
---------------------------------------------------------
Collections.shuffle(array);             // make array in random order

Grafana & Prometheus
https://www.youtube.com/watch?v=w-c3KYKQQfs
https://www.youtube.com/watch?v=lILY8eSspEo
https://www.youtube.com/watch?v=YUabB_7H710
AWS CloudWatch
https://www.youtube.com/watch?v=k7wuIrHU4UY
Kubernetes
https://www.youtube.com/watch?v=s_o8dwzRlu4
https://www.youtube.com/watch?v=TlHvYWVUZyc
NoSQL
https://www.youtube.com/watch?v=uD3p_rZPBUQ
Cassandra   - nosql database
https://www.youtube.com/watch?v=YjYWsN1vek8&list=PL2g2h-wyI4SqCdxdiyi8enEyWvACcUa9R
MongoDB     - nosql database
https://www.youtube.com/watch?v=bKjH8WhSu_E

OOP         -> CART -> Clarification, Assumption, Result, Test
System Design   -> SSSS Scenario, service, storage, Scale

Grafana is application for visualizing data, allows you to build charts, graphs and dashboards of data that you want to visualize. Grafana queries data source. data source return the required data.
There are two main categories of data that is visualized in grafana are metrics and logs. Common metrics: CPU load, current memory usage, temperature. Common log output: error message
Prometheus is data source

AWS CloudWatch is a system that helps you monitor the health of your applications that are built on aws. AWS CloudWatch Logs has two main components. The first one is Log groups and the second one is insights
Log groups are raw data dumps for your log files. but it is hard to read all the log one by one, that default filter is not working wells. Insights help to solve this and used to filter the log by using query
AWS CloudWatch Event used to create rule to invoke the lambda function
AWS Cloudwatch serviceLens is take a more granular look at what the invocations of system are spending their time on. for example, instance if you have a lambda function that writes to dynamo then publishes to an SNS topic.
ServiceLens allows you to do is kind of see how much of the total invocation duration for this lambda function is being spent on the dynamodb call and how much of that is being spent on the SNS
AWS CloudWatch Container Insights are using ecs elastic container service or elastic kubernetes service
AWS CloudWatch Synthetics we can kind of create canaries out of synthetics and what canaries allow you to do. monitor the health of your application through some periodic pings,
so you can kind of attach a canary to a certain api and keep on invoking that api every minute or on a frequent cadence that you specify and what this will allow you to do is ensure that this api is always up and running. This is way to monitor the health of your
service and also give you early warning system or early warning notifications. and tell you whether or not a system is working correctly
AWS CouldWatch Contributor Insights allows you to do time series analysis on your data, allows you to do grouping of data.

Kubernetes is an open-source container orchestration platform. It automates the deployment, scaling, and management of containerized application. The two core pieces in a Kubernetes cluster.
The first is control plane. it is responsible for managing the state of the cluster. In production environments, the control plane usually runs on multiple nodes that span across several data center zones.
Control Plane consists of a number of core components. They are API server, etcd, scheduler and the controller manager
API server is the primary interface between the control plane and the rest of the cluster
etcd is a distributed key-value store. it stores the cluster's persistent state. it is used by the API server and other components of the control plane to store and retrieve information about the cluster
Scheduler is responsible for scheduling pods onto the worker nodes in the cluster
The controller manager is responsible for running controllers that manage the state of the cluster
The second is a set of worker nodes. These nodes run the containerized application workloads. The containerized applications run in a Pod. Pds are smallest deployable units in Kubernetes. Pads are created and managed by the Kubernetes control plane.
The core components of Kubernetes that run on the worker nodes include kubelet, container runtime, and kube proxy
The kubelet is a daemon that runs on each worker node. It is responsible for communicating with the control plane. It receives instructions from the control plane about which pads to run on the node and ensures that the desired sate of the pads is maintained
The container runtime runs the containers on the worker nodes. It is responsible for pulling the container images from a registry, starting and stopping the containers, and managing the containers' resources.
The kube-proxy is a network proxy that runs on each worker node. It is responsible for routing traffic to the correct pods. It also provides load balancing for the pods and ensures that traffic is distributed evenly across the pods.
Pros:
Kubernetes is scalable and highly available. It provides features like self-healing automatic rollbacks, and horizontal scaling. It makes it easy to scale our applications up and down as needed, allowing us to respond to changes in demand quickly.
Kubernetes is portable. It helps us deploy and manage applications in a consistent and reliable way regardless of the underlying infrastructure. It runs on-premise, in a public cloud, or in a  hybrid environment. It provides a uniform way to package, deploy, and manage applications.
Cons:
drawback is complexity. It is complex to set up and operate. The upfront cost is high, especially for organizations new to container orchestration. It requires a high level of expertise and resources to set up and manage a production Kubernetes environment.
second draw back is cost. It requires a certain minimum level of resources to run in order to support all the features above. It is likely an overkill for manyu smaller organizations.
One popular option that strikes a reasonable balance is to offload the management of the control plane to a managed Kubernetes service. Managed Kubernetes services are provided by cloud providers. Likes, Amazon EKS, GKE on Google cloud, and AKS on Azure.
These services allow organizations to run the kubernetes application without having to worry about the underlying infrastructure. They take care of tasks that require deep expertise, like setting up and configuring the control plane, scaling the cluster, and providing ongoing maintenance and support.


